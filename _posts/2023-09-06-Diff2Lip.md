---
layout: post
title:  "Diff2Lip: Audio Conditioned Diffusion Models for Lip-Synchronization."
categories: 논문리뷰
date: 2023-09-06 11:40:18 +0900
tags: Diffusion 생성모델 arXiv
mathjax: true
author: Haribo
---
* content
{:toc}
**Full Citation**: "Mukhopadhyay, Soumik, et al. "Diff2Lip: Audio Conditioned Diffusion Models for Lip-Synchronization." *arXiv preprint arXiv:2308.09716* (2023)."\
**Link to Paper**: [https://arxiv.org/pdf/2308.09716.pdf](https://arxiv.org/pdf/2308.09716.pdf)\
**Conference Details**: arXiv 2023\
**Project Page**: [Link](https://soumik-kanad.github.io/diff2lip/)

---

> Lip synchronization task
>
> * Audio에 맞게 사람의 입술 움직임을 합성하는 task.
>
> * 영화 산업(더빙), 가상 아바타 등에서 다양한 응용이 가능하다.
>
> * 도전과제
>   * 디테일한 입술 움직임 구현
>
>   * identity, pose, emotions 등 source의 특징을 보존해야함

<div class="responsive-video-container" style="text-align:center;">
    <iframe src="https://soumik-kanad.github.io/diff2lip/static/website_videos/hp1.mp4" frameborder="0" style="margin: 0 auto; display: block;" allowfullscreen></iframe>
</div>






굉장히 재미있어 보이는 분야가 있어서 읽어봄.

위의 비디오 예시처럼 오디오에 맞게 사람의 입술 움직임을 합성하는 task인데 Lip synchronization task라고 한다.

ChatGPT로 시나리오랑 text 짜달라고 한뒤 [Text-to-Video](https://arxiv.org/abs/2209.14792) + Lip synchronization + [Video-to-Audio](https://text-to-audio.github.io/) 3개면 영화나 deepfake, 양산형 유투브 쇼츠 비디오 뚝딱 만들 수 있을 듯...

* 쇼츠공장으로 월 몇백 버는 사람 있다고하던데;



# Introduction

립 싱크(lip-synchronization)는 다른 speech audio에 맞게 사람의 입술 움직임을 합성하는 task.

이 과정에서 중요한 것은 자연스러운 입술 움직임만 합성하는 것이 아니라, 사용자의 identity, pose 등도 동시에 유지되어야 한다.

Video frame 처리 작업이 포함되어 있기 때문에, 4차원의 입력값에 연산이 필요하다.

- 입력 size: (C, F, H, W)
  - C: Channel, F: Frame, H: Height, W: Width

본 논문, 'Diff2Lip'은 위 립 싱크 task를 해결하기 위해 다음과 같은 구성 요소를 사용함:

- Pose context를 얻기 위한 **masked input** frame.
- Identity와 mouth region textures를 얻기 위한 **reference frame**.
- Lip shape를 제어하기 위한 **audio frame**.

## Contributions

Frame 차원이 포함된 입력 데이터에서 diffusion 기법을 사용하여 립 싱크를 수행한 'Diff2Lip'의 주요한 contribution은 아래와 같다:

* Audio-conditioned image generation을 위한 새로운 diffusion model 기반의 접근법을 제안.
* Frame-wise 및 sequential losses를 통해 고품질의 립 싱크를 성공적으로 수행.
* Sequential adv loss를 활용하여 diffusion 모델을 이용한 frame-wise video 생성을 더욱 stable하게 수행.

# Methods

## Diffusion Models

Diffusion 모델은 데이터의 복잡한 분포를 모델링하기 위해 확률적 확산 과정을 사용한다. 

주요 아이디어는 원본 데이터에 노이즈를 점진적으로 추가하고, 그 노이즈를 점차 제거하여 원래의 데이터를 재구성한다.

**노이즈 추가 과정**:

* $$x_t = \sqrt{ \bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, \epsilon \in N(0, I)$$
* $$\left\{ \bar{\alpha}_t \right\}_{t=1}^{T}$$: list of noise scales
* $$x_0$는 원본 데이터

**제거할 예측 noise에 대한 variational lower bound on the maximum likelihood objective:**

* $$L_{simple} = \mathbb{E}_{x_0, t, \epsilon}\left [ \left\| \epsilon_\theta (x_t, t)-\epsilon \right\|_2^2 \right ]$$

**노이즈 제거 과정 (denoising), posterior sampling:**

* $$x_{t-1} =  \sqrt{\frac{\bar{\alpha}_{t-1}}{\bar{\alpha}_{t}}} + (\sqrt{1-\bar{\alpha}_{t-1}}-\sqrt{\frac{\bar{\alpha}_{t-1}(1-\bar{\alpha}_{t})}{\bar{\alpha}_{t}}})\cdot \epsilon$$



자세한 Diffusion model은 [DDPM 논문](https://arxiv.org/abs/2006.11239) 참고

## Proposed Approach

ㄴ

#### Additional Losses

# Experiments

## Datasets

## Comparison Methods

## Quantitative Evaluation

## Qualitative Evaluation

## Ablations

