---
layout: post
title:  "MNIST with Neural Net"
categories: ML AI
date:   2021-04-10 15:10:18 +0900
tags: Math
mathjax: True
author: Haribo
---
* content
{:toc}
pytorch를 이용한 1 ~ 5층 까지의 신경망 구성해보기.  

[참고 사이트](https://tutorials.pytorch.kr/beginner/nn_tutorial.html)







# MNIST DataSet Load

jupyter notebook에서 MNIST데이터를 받으려 했지만, 실행이 되지않아 colab에서 MNIST 데이터를 받는 방식으로 했다.


```python
import gzip
import os
import sys
import struct
import numpy as np

def read_image(fi):
    magic, n, rows, columns = struct.unpack(">IIII", fi.read(16))
    assert magic == 0x00000803
    assert rows == 28
    assert columns == 28
    rawbuffer = fi.read()
    assert len(rawbuffer) == n * rows * columns
    rawdata = np.frombuffer(rawbuffer, dtype='>u1', count=n*rows*columns)
    return rawdata.reshape(n, rows, columns).astype(np.float32) / 255.0

def read_label(fi):
    magic, n = struct.unpack(">II", fi.read(8))
    assert magic == 0x00000801
    rawbuffer = fi.read()
    assert len(rawbuffer) == n
    return np.frombuffer(rawbuffer, dtype='>u1', count=n)

if __name__ == '__main__':
    os.system('wget -N http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz')
    os.system('wget -N http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz')
    os.system('wget -N http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz')
    os.system('wget -N http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz')
    
    np.savez_compressed(
        'mnist',
        train_x=read_image(gzip.open('train-images-idx3-ubyte.gz', 'rb')),
        train_y=read_label(gzip.open('train-labels-idx1-ubyte.gz', 'rb')),
        test_x=read_image(gzip.open('t10k-images-idx3-ubyte.gz', 'rb')),
        test_y=read_label(gzip.open('t10k-labels-idx1-ubyte.gz', 'rb'))
    )
```


```python
data = np.load('mnist.npz')
x_train = data['train_x']
y_train = data['train_y']
x_test = data['test_x']
y_test = data['test_y']
```

## Data Reshape

이 방식으로 데이터를 받으면 `(60000, 28, 28)` 형태의 데이터로 로드되기 때문에 `x` 데이터들을 1차원 벡터형태로 펴준다.


```python
# x_train.shaep : (60000, 28, 28) -> (60000, 784)
# x_test.shape : (10000, 28, 28) -> (10000, 784)
x_train = x_train.reshape((60000, 784))
x_test = x_test.reshape((10000, 784))
```

## to pytorch tensor

`pytorch`를 이용해 신경망을 구성할것이기 때문에 `torch.tensor`로 변경해준다.  

이유는 모르겠지만 `y`데이터의 형태를 `long`으로 바꾸지 않으면 선형 계산 과정에서 예외가 뜨며 실행이 되지않는다.


```python
import torch

x_train, y_train, x_test, y_test = map(
    torch.tensor, (x_train, y_train, x_test, y_test)
)
# y를 long으로 바꾸지 않으면 index에러같은게 떠서 바꿨습니다.
y_train = y_train.long()
y_test = y_test.long()
```

# Neural Net implement

hypter parameter 튜닝을 위해 `GPU`로 바꿔보려 했지만 계속 예외가 떠서 `GPU`는 잠시 보류했다.


```python
import torch.nn.functional as F
from torch import nn
from torch import optim
from torch.utils.data import TensorDataset
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import time


# GPU로 바꾸려했지만, 
# RuntimeError: Tensor for argument #2 ‘weight’ is on CPU, but expected it to be on GPU (while checking arguments for cudnn_batch_norm) 가 떠서 해결법을 못찾음
print(torch.cuda.is_available())
dev = torch.device(
    "cuda") if torch.cuda.is_available() else torch.device("cpu")
```

    True

---

## MyModule

인자로 `layer`의 갯수를 받을 수 있도록 `nn.ModuleList()`를 이용해 설계를 하였다.

```python
# Cross entropy loss func
loss_func = F.cross_entropy

def accuracy(out, yb):
    preds = torch.argmax(out, dim=1)
    return (preds == yb).float().mean()
  
class MyModule(nn.Module):
    def __init__(self, layer = 1):
        super().__init__()
        self.max_layer = 512
        self.relu = nn.ReLU()
        self.log_softmax = nn.LogSoftmax()
        self.linears = nn.ModuleList()
        '''
        layer = 1 : 784 x 10
        layer = 2 : 784 x 512 x 10
        layer = 3 : 784 x 512 x 256 x 10
        layer = 4 : 784 x 512 x 256 x 126 x 10
        layer = 5 : 784 x 512 x 256 x 126 x 64 x 10
        '''
        if layer > 1 :
            self.linears.append(nn.Linear(784, self.max_layer))
            for i in range(2, layer) :
                self.linears.append(nn.Linear(self.max_layer, self.max_layer // 2))
                self.max_layer //= 2
            self.linears.append(nn.Linear(self.max_layer, 10))
        else :
            self.linears.append(nn.Linear(784, 10))
    def forward(self, x):
        for linear in self.linears[:-1] :
            x = self.relu(linear(x))
        # output layer
        x = self.log_softmax(self.linears[-1](x))
        return x

# model, optim 설정
def get_model(layer = 1, lr = 0.01):
    model = MyModule(layer)
    return model, optim.SGD(model.parameters(), lr=lr)

'''
fit의 가독성을 위해 Dataset, DataLoader 모듈을 이용해
batch_size 크기씩 뽑아올 수 있게 iterator 객체로 만들어 준다.
'''
def get_data(train_ds, test_ds, bs):
    return (
        DataLoader(train_ds, batch_size=bs, shuffle=True),
        DataLoader(test_ds, batch_size=bs * 2),
    )

'''
test 데이터와, train 데이터일때의 구별을 하기위해 opt = None으로 설정
test 데이터에서 쓸데없이 parameter 업데이트가 되지 않도록 하기 위해서
'''
def loss_batch(model, loss_func, xb, yb, opt=None):
    loss = loss_func(model(xb), yb)
    if opt is not None:
        loss.backward()
        opt.step()
        opt.zero_grad()
    return loss.item()

'''
fit 함수
x_train, y_train으로 학습을 끝낸 후
train, test의 정확도를 출력
'''
def fit(epochs, model, loss_func, opt, train_dl, test_dl):
    start = time.time()
    train_loss = []
    for epoch in range(epochs):
        model.train()
        for xb, yb in train_dl:
            loss = loss_batch(model, loss_func, xb, yb, opt)
        train_loss.append(loss)
        if (epoch + 1) % 10 == 0: 
            print('{} epoch is done'.format(epoch + 1))
    print('train accuracy : ', accuracy(model(x_train), y_train))  
    print('test accuracy : ', accuracy(model(x_test), y_test)) 
    print("cost time :", time.time() - start)       
    return train_loss
```


```python
batch_size = 1024
epochs = 200
lr = 0.05
# Dataset 정리
train_ds = TensorDataset(x_train, y_train)
test_ds = TensorDataset(x_test, y_test)
train_dl, test_dl = get_data(train_ds, test_ds, batch_size)
```

# Layer 갯수와 accuracy 비교

Layer의 갯수와 accuracy 관계를 살펴보겠다.

## Layer : 1


```python
model, opt = get_model(layer = 1, lr = lr)
train_loss = fit(epochs, model, loss_func, opt, train_dl, test_dl)
```

    10 epoch is done
    20 epoch is done
    30 epoch is done
    40 epoch is done
    50 epoch is done
    60 epoch is done
    70 epoch is done
    80 epoch is done
    90 epoch is done
    100 epoch is done
    110 epoch is done
    120 epoch is done
    130 epoch is done
    140 epoch is done
    150 epoch is done
    160 epoch is done
    170 epoch is done
    180 epoch is done
    190 epoch is done
    200 epoch is done
    train accuracy :  tensor(0.9222)
    test accuracy :  tensor(0.9223)
    cost time : 104.36



```python
plt.plot(train_loss)
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.show()
```


![png](/images/mnist/output_11_0.png)
    

## Layer : 2


```python
model, opt = get_model(layer = 2, lr = lr)
train_loss = fit(epochs, model, loss_func, opt, train_dl, test_dl)
```

    10 epoch is done
    20 epoch is done
    30 epoch is done
    40 epoch is done
    50 epoch is done
    60 epoch is done
    70 epoch is done
    80 epoch is done
    90 epoch is done
    100 epoch is done
    110 epoch is done
    120 epoch is done
    130 epoch is done
    140 epoch is done
    150 epoch is done
    160 epoch is done
    170 epoch is done
    180 epoch is done
    190 epoch is done
    200 epoch is done
    train accuracy :  tensor(0.9813)
    test accuracy :  tensor(0.9726)
    cost time : 415.87



```python
plt.plot(train_loss)
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.show()
```


![png](/images/mnist/output_14_0.png)
    

## Layer : 3


```python
model, opt = get_model(layer = 3, lr = lr)
train_loss = fit(epochs, model, loss_func, opt, train_dl, test_dl)
```

    10 epoch is done
    20 epoch is done
    30 epoch is done
    40 epoch is done
    50 epoch is done
    60 epoch is done
    70 epoch is done
    80 epoch is done
    90 epoch is done
    100 epoch is done
    110 epoch is done
    120 epoch is done
    130 epoch is done
    140 epoch is done
    150 epoch is done
    160 epoch is done
    170 epoch is done
    180 epoch is done
    190 epoch is done
    200 epoch is done
    train accuracy :  tensor(0.9946)
    test accuracy :  tensor(0.9786)
    cost time : 578.93



```python
plt.plot(train_loss)
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.show()
```


![png](/images/mnist/output_17_0.png)
    

## Layer : 4


```python
model, opt = get_model(layer = 4, lr = lr)
train_loss = fit(epochs, model, loss_func, opt, train_dl, test_dl)
```

    10 epoch is done
    20 epoch is done
    30 epoch is done
    40 epoch is done
    50 epoch is done
    60 epoch is done
    70 epoch is done
    80 epoch is done
    90 epoch is done
    100 epoch is done
    110 epoch is done
    120 epoch is done
    130 epoch is done
    140 epoch is done
    150 epoch is done
    160 epoch is done
    170 epoch is done
    180 epoch is done
    190 epoch is done
    200 epoch is done
    train accuracy :  tensor(0.9995)
    test accuracy :  tensor(0.9795)
    cost time : 622.08



```python
plt.plot(train_loss)
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.show()
```


![png](/images/mnist/output_20_0.png)
    

## Layer : 5


```python
model, opt = get_model(layer = 5, lr = lr)
train_loss = fit(epochs, model, loss_func, opt, train_dl, test_dl)
```

    10 epoch is done
    20 epoch is done
    30 epoch is done
    40 epoch is done
    50 epoch is done
    60 epoch is done
    70 epoch is done
    80 epoch is done
    90 epoch is done
    100 epoch is done
    110 epoch is done
    120 epoch is done
    130 epoch is done
    140 epoch is done
    150 epoch is done
    160 epoch is done
    170 epoch is done
    180 epoch is done
    190 epoch is done
    200 epoch is done
    train accuracy :  tensor(1.0000)
    test accuracy :  tensor(0.9787)
    cost time : 633.75



```python
plt.plot(train_loss)
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.show()
```


![png](/images/mnist/output_23_0.png)
    


# 종합적인 비교
> 각 layer 노드가 많아 시간이 오래걸려 하이퍼파라미터 튜닝을 많이 못해봤지만 이것저것 건들여 보면서 느낀것이 확실이 **Layer가 많을수록 성능이 좋아질 확률이 있다.** 대신 Layer가 많으면 epoch를 더 많이 주어야한다.
>
> * epoch : 100 일 때, Layer 5의 accuracy 성능 (0.9420, 0.9361)
> * epoch : 200 일 때, Layer 5의 accuracy 성능 (1.0000, 0.9787)
>
> 정확도가 1이 나올 수 있다는 것이 참 놀랍다.
>
> * 하지만 오버피팅임을 알 수 있다.
>
> 확실이 Machine Learning 모델(SVM, Logistic Regression 등등)로 돌렸을 때보다 성능이 많이 좋은것이 체감이되고, torch.nn 모듈 사용으로 인해 코드가 정말 간결해졌다.  
>


```python
train_accuracy = [0.9222, 0.9813, 0.9946, 0.9995, 1.0000]
test_accuracy = [0.9223, 0.9726, 0.9786, 0.9795, 0.9787]
x = [1, 2, 3, 4, 5]
# summarize history for accuracy
plt.plot(x, train_accuracy)
plt.plot(x, test_accuracy)
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('Layer')
plt.xticks(x)
plt.legend(['train', 'test'], loc='upper left')
plt.show()
```


![png](/images/mnist/output_25_0.png)
    