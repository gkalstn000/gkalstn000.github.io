---
layout: post
title:  "Emu: Enhancing Image Generation Models Using Photogenic Needles in a Haystack"
categories: 논문리뷰
date: 2023-10-18 11:40:18 +0900
tags: Diffusion 생성모델 Meta
mathjax: True
author: Haribo
---
* content
{:toc}
**Full Citation**: "Dai, Xiaoliang, et al. "Emu: Enhancing image generation models using photogenic needles in a haystack." arXiv preprint arXiv:2309.15807 (2023)."\
**Link to Paper**: [https://arxiv.org/abs/2309.15807](https://arxiv.org/abs/2309.15807) \
**Conference Details**: Meta 2023

---

> * 수작업으로 선택 된 **high quality 이미지(highly aesthetically-pleasing)**는 text-to-image 생성 모델에서 이미지의 미적성(aesthetics)을 향상시킬 수 있다. 
>
> * 단지 수백에서 수천 개의 high quality 이미지를 fine-tuning하면 생성된 이미지의 시각적 매력이 향상된다.
>
> * 이러한 quality-tuning은 Latent Diffusion Model(LDM)뿐만 아니라 Pixel Diffusion 및 masked generative transformer models에도 효과가 있다.



<div style="text-align: center;">   
  <figure>     
    <img src="/images/Emu/image-20231024205421674.png" alt="image-20231024205421674">     
    <figcaption>Emu generated image samples</figcaption>   
  </figure> 
</div>


# Introduction

최근의 생성 모델 발달로 고품질 컨텐츠 생성이 가능해졌다.

* Text, Image, Music, Video, 3D scenes 등등.

본 논문에서는 텍스트를 입력으로 사용하여 매우 뛰어난 미적 이미지를 생성하는 text-to-image 학습 방법을 소개한다.

이 방식은 크게 두 단계로 구성된다:

1. **Knowledge learning stage**: 수십억장의 text-image pair로 Diffusion 생성모델을 학습.
2. **Quality learning stage:** Diffusion 생성모델의 결과물을 high-quality & aesthetically pleasing한 domain으로 제한하여(tuning) 형성한다.



즉, 수십억개의 데이터셋으로 이미 학습된 text-to-image 모델은, 몇 천장의 엄선된 high-quality & aesthetically pleasing한 이미지로 fine-tuning을 통해 output domain을 high-quality & aesthetically pleasing domain으로 제한할 수 있다.



<div style="text-align: center;">   
  <figure>     
    <img src="/images/Emu/image-20231024212318577.png" alt="image-20231024212318577">     
    <figcaption>Pre-trained LDM vs Quality-tuned LDM</figcaption>   
  </figure> 
</div>

# Approach

**Introduction**에서 discuss했듯이 본 논문에서는 knowledge learning stage 다음 quality-tuning stage를 제안한다. 하지만 이는 이미 잘 알려진 fine-tuning과 다를바 없어 보이지만 아래와 같은 큰 차이점이 존재한다.

* Fine-tuning에 필요한 데이터셋의 크기는 상대적으로 작다 (몇백장에서 몇천장 사이).
* Fine-tuning에 사용되는 데이터는 매우 높은 품질을 보유하고 있다.
* 수백장 혹은 수천장의 고품질 데이터로 fine-tuned된 모델은 이미지 품질의 향상과 함께, 원래 모델의 input prompt의 정확성도 유지한다.

## Latent Diffusion Architecture

Pre-trained model은 [Stable Diffusion](https://arxiv.org/abs/2112.10752)모델의 구조를 baseline으로 삼고 약간의 튜닝을 하였음.

* 기존의 4-channel autoencoder 구조를 지닌 Stable Diffusion 모델의 channel을 16channel로 변경

  * <div style="text-align: center;">   
      <figure>     
        <img src="/images/Emu/image-20231024213935670.png" alt="image-20231024213935670.png">     
        <figcaption>LDM 모델의 channel 수를 늘리면 더 좋은 품질이 reconstruction 됨.</figcaption>   
      </figure> 
    </div>

* Reconstruction quality를 향상시키기 위해 adversarial loss와 더불어 *Fourier Feature Transform*을 활용해 non-learnable pre-processing step을 추가하였음.

  * <div style="text-align: center;">   
      <figure>     
        <img src="/images/Emu/image-20231024214210286.png" alt="image-20231024214210286.png">     
        <figcaption>LDM 모델의 channel 수 변화 및 Fourier Feature Transform 사용에 따른 성능 향상 수치</figcaption>   
      </figure> 
    </div>



## Pre-training

1.1 billion개의 text-image pair 데이터셋으로 LDM 모델 학습.

이 때 고화질 이미지 생성을 위해 [SDLX](https://arxiv.org/abs/2307.01952)와 동일한 방식으로 학습 진행.



## High-Quality Alignmemt Data

<div style="text-align: center;">   
  <figure>     
    <img src="/images/Emu/image-20231024214935394.png" alt="image-20231024214935394.png">     
    <figcaption>High-Quality Alignment Dataset 선별 과정</figcaption>   
  </figure> 
</div>

본 논문에서는 Quality-tuning을 위한 이미지 선별 과정을 두 단계로 제안.

### Automatic Filtering

초기 수십억 장의 이미지 풀 중에서, 유해한 이미지, OCR, 그리고 CLIP 점수(이미지와 prompt 사이의 연관성)를 기준으로 선별하여, 이미지의 수를 200K개로 축소.

### Human Fintering

사진과 예술 분야의 전문가들을 대상으로, 아래의 기준들을 제시하였고, 이 기준에 따라 이미지를 선별하게 하여 총 2000장의 미적 특성을 가진 이미지를 선정.

* Composition

* Lighting

* Color and Contrast

* Subject and Background

* Additional Subjective Assessment

<div style="text-align: center;">   
  <figure>     
    <img src="/images/Emu/image-20231024215720449.png" alt="image-20231024215720449.png">     
    <figcaption>최종적으로 선정 된 High-Quality Alignmemt 이미지 예시</figcaption>   
  </figure> 
</div>

## Quality-Tuning

**저자들의 주장:**

시각적으로 뛰어난 이미지들(우리가 수집한 2000개의 이미지와 같은)은 전체 이미지 중의 일부 부분집합으로서, 특정한 공통 통계적 특성을 가지고 있다.

강력하게 사전 훈련된 모델은 이미 아름다운 이미지를 생성할 능력이 있지만, 생성 과정에서 항상 이러한 공통 통계적 특성을 갖는 이미지를 생성하도록 제대로 안내되지 않는다.

그러나 Quality-tuning을 통해 시각적으로 뛰어난 이미지의 통계적 특성을 캡쳐하고, 높은 품질의 이미지 부분집합만 생성하도록 모델의 출력을 제한할 수 있다.

**Fine-tuning의 세부 사항:**

조기 종료(Early stopping)는 매우 중요하다. Fine-tuning을 과도하게 수행하면, 텍스트와 이미지 간의 일치성과 같은 일반화 성능이 저하될 수 있다. 따라서 loss가 감소하더라도 15K 반복 이후에는 훈련을 중단한다.

# Experiments

Quality-tuning의 효과를 검증하기 위해 다음과 같은 실험 구성을 진행.

* *Pre-trained Model* VS *Quality-tuned Model*
* *SOTA([SDXLv1.0](https://arxiv.org/abs/2307.01952))* VS *Quality-tuned Model*
* Latent Diffusion Model 이외에도 Pixel Diffusion Model과 Masked Generative Transformer Model(GAN 기반)에 Quality-tuning의 효과가 있는지 실험.

## Evaluatio Setting

### Prompts

* [PartiPrompts](https://arxiv.org/abs/2206.10789): 텍스트-이미지 생성 벤치마킹을 위한 1600개의 프롬프트
* Open User Input(OUI): 실제 사용자들이 주로 사용하는 프롬프트 2100개.

<div style="text-align: center;">   
  <figure>     
    <img src="/images/Emu/image-20231024222114592.png" alt="image-20231024222114592.png">     
    <figcaption>OUI prompt distribution</figcaption>   
  </figure> 
</div>

### Metrics

평가 기준은 이미지의 visual appealing과 text faithfulness에 초점을 둠.

* Visual Appealing: text prompt를 보여주지 않고 이미지 자체로만 평가하도록 지시.
* Text Faithfulness: Image와 text prompt를 둘다 보여준 후 이미지 퀄리티에 상관없이 text와 이미지의 faithfulness를 평가하도록 지시.

5명의 annotator에게 "A", "B" 두장의 이미지를 보여준 후 각 평가 기준에 부합한 이미지를 선택하도록 함. 이 때 둘다 별로거나 둘다 좋은경우 "Tie"로 평가 됨.

"A" + "B" + "Tie"의 합계는 항상 100.

그 외에 FID, LPIPS 등 생성 모델의 성능을 평가할 때 사용하는 metric은 사용하지 않음.

- 정확한 Ground Truth가 없기 때문에



## Results

### Effectiveness of Quality-Tuning

<div style="text-align: center;">   
  <figure>     
    <img src="/images/Emu/image-20231024223023107.png" alt="image-20231024223023107.png">     
    <figcaption>Pre-trained VS Quality-trained</figcaption>   
  </figure> 
</div>

* Quality-tuned model is preferred in both visual appeal and text faith-fulness

* Stylized Prompts: non-photorealistic stylized prompts (e.g., sketches, cartoons, etc.)

**실험 결과 분석:**

Visual Appeal에서 Quality-tuned 모델이 대부분의 평가에서 선호되었음. "Tie"를 선택한 평가자는 거의 없음.

반면, Text Faithfulness에서는 Quality-tuned 모델을 선호한 평가자도 많았지만, "Tie"를 평가한 사람의 수도 상당히 많았음.

- "Tie" 평가는 두 모델이 비슷한 성능을 보일 때 선택됨.

이 결과는 Quality-tuned 모델이 시각적으로 pre-trained 모델보다 뛰어나면서도, 텍스트 충실도(text-faithfulness) 측면에서 pre-trained 모델의 능력을 잃지 않았음을 나타냄.

<div style="text-align: center;">   
  <figure>     
    <img src="/images/Emu/image-20231024212318577.png" alt="image-20231024212318577">     
    <figcaption>Pre-trained LDM vs Quality-tuned LDM</figcaption>   
  </figure> 
</div>

### Visual Appeal in the Context of SOTA

<div style="text-align: center;">   
  <figure>     
    <img src="/images/Emu/image-20231024224100263.png" alt="image-20231024224100263.png">     
    <figcaption>SOTA vs Quality-tuned Model</figcaption>   
  </figure> 
</div>

* To place the visual appeal of Emu in the context of current SoTA model, SDXL v1.0.

* Visual appeal 면에서 현 SOTA 모델인 SDXL v1.0 보다 훨 씬 우수한 평가를 받음.



### Quality-Tuning Other Architectures

Latent Diffusion Model 뿐만 아니라 다른 형식의 text-to-image 모델에 대해서도 quality-tuning 실험 진행.

* [Imagen](https://arxiv.org/abs/2205.11487): Pixel Diffusion Model
* [Muse](https://arxiv.org/abs/2301.00704): Masked Generative Transformer Model
  * GAN based text-to-image 모델

<div style="text-align: center;">   
  <figure>     
    <img src="/images/Emu/image-20231024231035742.png" alt="image-20231024231035742.png">     
    <figcaption>Quality-Tuning vs Pre-training on Pixel Diffusion and Masked Generative Transformer.</figcaption>   
  </figure> 
</div>

Latent Diffusion Model 만큼의 성능 향상을 보여주진 않지만 꽤나 효과적인 성능향상과 text-faithfulness 능력이 유지됨을 보여줌.



### Ablation Study

이미지 개수를 조절하여 Quality-tuning의 성능평가 실험을 진행.

<div style="text-align: center;">   
  <figure>     
    <img src="/images/Emu/image-20231024231330909.png" alt="image-20231024231330909.png">     
    <figcaption>Visual Appeal by Fine-Tuning Dataset Size.</figcaption>   
  </figure> 
</div>

고품질의 이미지 100장만 quality-tuning에 사용해도 엄청난 효과가 있음.

* 대략 35% 선호도 증가.

# Limitation

**Limitation of Human Evaluation.**

* 최근에 발표된 연구들처럼 다양한 합리적인 프롬프트에 대한 실험을 진행했지만, 완전한 현실적 실용성을 반영하는 것은 쉽지 않다.
* 전문가들의 평가는 주관적일 수 있으며, 때로는 잡음이 포함될 수 있다.

**Limitations of Small-Scale Fine-Tuning.**

* 고품질의 이미지를 소량으로 fine-tuning했기 때문에, 기존의 pre-trained 모델에서 발생하는 근본적인 문제점들이 여전히 존재할 가능성이 있다.

# Conclusion

-  **high quality images**를 수동으로 선택하는 것은 **text-to-image generative models**로 생성된 이미지의 미학적인 품질을 향상시키는데 있어 **가장 중요한 핵심** 중 하나이다.
- 단지 **몇 백에서 천 개의 fine-tuning 이미지**만으로도 생성된 이미지의 시각적 매력을 향상시킬 수 있었다.
- 품질 튜닝은 LDM 뿐만 아니라 Pixel Diffusion Model 과 Masked Generative Transformer Model 도 개선한다.

# 개인 Review

* Pre-trained 모델에 사용된 이미지 풀에서 고품질의 이미지를 선택했는지, 아니면 별도로 수집한 이미지 풀에서 선택했는지에 대한 정보가 부족하다.
  - Pre-trained 모델에 사용된 이미지 풀에서 고품질 이미지를 선택했다면, 저자들의 주장대로 이미 고품질 이미지의 통계는 학습되었을 것이다. 그러나 대다수의 일반 품질의 이미지로 인해 학습이 조정되면서 고품질 이미지 생성 능력이 감소되었을 수 있고, fine-tuning을 통해 다시 이를 향상시키고자 하는 접근이 타당하다고 생각될 수 있다.
* text-to-image 작업 외에도 다른 종류의 작업에서 (예를들면 image-to-image translation, object detection, classification, segmentation 등등) quality-tuning의 효과가 있는지에 대해 궁금함.
* 본 논문은 고품질 이미지의 선택이 전문가의 주관적인 판단에 기반하였지만, 고품질의 기준을 수학적으로 정의하고 해당 기준에 따라 이미지를 선택하여 fine-tuning한 결과는 어떠했을지 궁금함.



# Additional Generated Examples

<div style="text-align: center;">   
  <figure>     
    <img src="/images/Emu/image-20231024232908462.png" alt="image-20231024232908462.png">     
    <figcaption>Generated Examples.</figcaption>   
  </figure> 
</div>

<div style="text-align: center;">   
  <figure>     
    <img src="/images/Emu/image-20231024232933608.png" alt="image-20231024232933608.png">     
    <figcaption>Generated Examples.</figcaption>   
  </figure> 
</div>

<div style="text-align: center;">   
  <figure>     
    <img src="/images/Emu/image-20231024233001044.png" alt="image-20231024233001044.png">     
    <figcaption>Generated Examples.</figcaption>   
  </figure> 
</div>
