---
layout: post
title:  "AI 기강 다지기"
categories: 실습
date: 2025-10-29 11:40:18 +0900
tags: AI Optimizer Diffusion
mathjax: True
author: Haribo
---
* content
{:toc}

직장생활 1년 10개월 정도하면서 프로젝트 완성에만 초점을 두느라 대충 슥 보고 넘어갔던 개념들이 많아 지나온길, 써봤던거 돌아보며 개념좀 다시 잡고자 포스팅하게되었음.  
특히 코드 보면서 "아 그런갑다~" 하고 넘어갔던 놈들 위주로 다시 리마인드 하면서 계속 적어나갈 예정





# 1. 학습/훈련 기초

## [Accelerater]({% post_url 2025-11-04-Accelerater %})
`kohya-ss`가 쓰는대로 따라만 썼지, 세팅하고 뭔 기능 있는지는 자세히 모름.
* accelerater config vs 실행전에 쓰는 mix-precision vs 코드 내부에서 세팅하는 precision
* auto-casting
* logger 세팅
* DDP, sync 등
* multi-node 세팅방법
* DeepSpeed 등등

## Optimizer (Prodigy, AdamW, Adafactor 등)
특히 `Prodigy`이놈이 `lr`자동으로 조절해주는 요물인데... 그러나 언제 `lr`이 올라가고 그런거를 모르고, 이거를 foundation 학습, adapter 학습, fine-tuning 등등 언제 써야할지 모름.

## [각종 Norm (Transformer에서 많이쓰는 RMSNorm, LayerNorm 같은거), Modulation]({% post_url 2025-11-16-여러 Norm들 %})
이전에는 batch_norm, instance_norm 등등 분쟁이 많았었는데 요즘은 
* RMSNorm, LayerNorm 
* Conditional Norm (Modulation)

이걸로 굳혀진듯ㅇㅇ

# 2. 모델 구조 및 표현

## FlashAttention
대학원 때까지만 해도 어텐션은 `1024` 길이까지만 감당 가능했는데 어느순간부터 그거보다 훨씬 긴 길이도 되는거 대수롭지 않게 넘겼지만, `attention matrix` 를 건드려야하는데 내부에서 연산하고 결과만 돌려주다보니 뭔지 좀 알아둘 필요성을 느낌.

## Position Embeddings 
이미지는 일단 무조건 3D RoPE를 쓰니까 알아둬야함.

## 각종 경량화 기법 & LoRA
진자 개빡치는 요소 중 하나. 학습 때 모니터링 하면 문제 없었는데 `bfloat16` -> `fp8`로 바꾸면 생성 퀄리티 씹창나는 경우가 많음.
특히 LoRA rank, alpha 에 따라 학습 후 artifact 여부가 상당히 달라짐.


# 3. Diffusion

## Rectified-Flow
언젠가 손봐주려고 했던 놈이긴 한데, 이참에 테이크다운 들어가봐야겠음.

## Distillation (LADD, Guidance-distill)
이거만 제대로 해도 무거운 모델 서비스용으로 만들기 좋을듯.

## Video 생성모델
데이터 수집은 당연히 오픈 안되어있을꺼고, 비디오, 소리, 어떻게 끌어다 쓰는지 `dataloader` 구조, caption은 어떻게 만드는지/결과물 어케 생겼는지, 하이퍼 파라미터들 알아둘 필요있음.

## Sampler
shift, mu 같은거 대략 알겠는데, 코드를 좀 봐야겠음.


# 4. 학습의 마무리 DPO, GRPO

# 5. 그 외

## LLM, VL 맛보기

## 각종 이미지 생성/편집 벤치마크 

## 학습 프로젝트 패키지 구조 생각하기
패키지 관리, 룰 같은거 안만들어 두면 순환 참조, 코드 개판, 확장, 협업 개어려워짐.  
물론 지금 나만의 룰이 있는데 빈약하기 그지 없음.

## 멀티 스레드 처리
데이터 전처리 할 때 맨날 쓰는데 gpt한테 시켜서 정작 나는 짤 줄 모름.

## 동기/비동기 함수
