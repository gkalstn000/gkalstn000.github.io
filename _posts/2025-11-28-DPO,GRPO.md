---
layout: post
title:  "DPO와 아이들 (GRPO)"
categories: 실습
date: 2025-11-28 11:40:18 +0900
tags: AI DPO 학습 training
mathjax: True
author: Haribo
---
* content
{:toc}

![casting.png](/images/DPO/comp.png)

subject-driven 모델 개발하던 중 보게 된 선행연구 [NEGATIVE-GUIDED SUBJECT FIDELITY OPTIMIZATION FOR ZERO-SHOT SUBJECT-DRIVEN GENERATION](https://arxiv.org/pdf/2506.03621).  
DPO 라는걸 임커밋 오픈채팅방에서 누가 쏼라쏼라하던거 말로만 듣고 먼지도 모르는 상태에서 실험 결과를보고 안써볼 수 가 없었다. 어차피 학습 파이프라인 0-100 혼자 만들어둔 상태였기에 DPO 학습을 위한 dataloader, loss func 부분 추가하는건 그렇게 어려워 보이지 않아 바로 테스트해본 결과가 위의 이미지다(본인 와꾸임).  






학습 데이터 수집을 위해 외주 맡긴 업체 ~~씨발년들이~~ 핸드폰으로 스크린샷 찍어서 위아래양옆 패딩 되어있고, 작은 이미지 사이즈 upscale해서 화질 개박살난 이미지, group 개판으로 지좆대로 섞어두고 이런 개쓰레기 데이터셋이 거의 반넘게 포진되어있던 터라 필터링하고 분류하는데만 거의 3주 넘게 썼다. 그럼에도 데이터셋이 워낙 개쓰레기라 학습 결과물이 처참했다 (데이터셋 생각하면 지금도 쳐죽이고싶음 ㄹㅇ).  
* 강제 업스케일 된 이미지 영향을 받아 간혹 뿌옇게 된 이미지가 생성됨.
* foundation 모델의 한계로 한국 고유 domain 생성 능력 떨어짐 (한국 모델, 닭발, 뼈해장국, 냉면 등등).
* 상품설명, 조그만 제품이름 같은 작은 한글 텍스트 깨짐 (큰건 그나마 ㄱㅊ)

그런데 이게 왠걸, 먼지도 모르는 DPO 썼더니 한글텍스트 부분 빼고 거의 완치되었음. 이건 도의적으로 읽어줄 수 밖에 없는 논문이었음.  
게다가 요즘 이미지 생성, 이미지 편집 모델은 대부분 학습 후 강화학습으로 한번 bake 해주는게 정배인 터라 꼭 이론적으로 알고있어야하는 연구라고 생각됨.  
근데 논문이 진짜 appendix에 친절하게 하나부터 열까지 차근차근 설명을 잘해줘서 이론 부분은 그렇게 어렵지 않았는데, 실험부분에 세팅을 개같이 꼬아놔서 너무 이해하기 힘들었음. 
* 실험 이해 어렵고 많은거 이건 자연어 논문 종특이라 진짜 짜증나는데 진짜 열심히 읽고 이해했다.

그리고 DPO의 아들인 GRPO도 살짝 맛만 보겠다.

---

그래서 이 포스트는 아래의 4가지에 대한 해답을 얻기 위함임.
* DPO가 그래서 원리가 뭔교?
* 어떻게 해야 극대화 할 수 있는교?
* 이미 DPO, RLHF로 학습 된 모델에 fine-tuning 같은거 해도됨?
* GRPO 얘는 머임

# DPO가 뭐임?

DPO논문이 근래 읽은 논문, 아니 여태껏 읽은 논문 중 개지렸던 top-3안에 든다.
* 내 픽: GAN, SCORE-BASED GENERATIVE MODELING ..., Auto-Encoding Variational Bayes, DDPM 등등

이 논문의 본질은 RLHF loss를 서커스해서 깔끼하게 바꾼거 보단 **PPO 같은 Actor-Critic 알고리즘의 암덩어리인 critic을 도려낸거**라 생각한다.  
Section 5에 그 내용이 자세히 나와있긴한데 이건 따로 DPO 포스트를 써야하는 양이라 이 포스트에서는 DPO 껍데기만 살짝 보겠다ㅇㅇ  

내가 생각한 DPO를 한줄로 요약하면 아래와 같다.  

> 모델의 고점은 유지하고 저점을 높여라

RLHF에서 태어난게 DPO이고, RLHF 자체가 "아 생성모델 임마 이거 진짜 빨딱선날은 좋은데 간혹 개 ㅂㅅ이 되네. 이거 ㅂㅅ짓좀 안하게 할수 없노?"를 겨냥하고 연구한걸로 이해했고 DPO도 수식을 보면 ㅂㅅ짓 못하게끔 수식이 구성되어있다.  

![casting.png](/images/DPO/rlhf.png)

우선 RLHF는 이렇게 생겼다. 
* 보상높은 output만 출력하도록 하면서: ㅂㅅ짓하지말고
* 학습할 모델 $\pi_{\theta}$는 기존모델 $\pi_{ref}$와 동떨진 출력 내면안된다: 잘하는거 (고점)는 유지해라

기존 RLHF (PPO)의 문제점은 학습 불안정 (critic) 및 복잡한 학습 과정이다. 자세한건 [DPO 리뷰]({% post_url 2025-11-30-DPO %}) 포스트를 확인하자.

$$
\mathcal{L}_{\text{DPO}}(\pi_{\theta}; \pi_{\text{ref}}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\pi_{\theta}(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_{\theta}(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)} \right) \right]
$$

이 수식이 RLHF 깍아서 만든 DPO loss 함수다. 이걸 업데이트 하기 위해 $\theta$로 미분 때리면 아래와 같은 형식이 나온다.

![gradient](/images/DPO/gradient.png)




# 그래서 DPO 어떻게 쓰라는거임
# DPO 된 foundation 모델에다가 학습 해도됨?
# GRPO
