---
layout: post
title:  "DPO와 아이들 (GRPO)"
categories: 실습
date: 2025-11-22 11:40:18 +0900
tags: AI DPO 학습 training
mathjax: True
author: Haribo
---
* content
{:toc}

![casting.png](/images/DPO/comp.png)

subject-driven 모델 개발하던 중 보게 된 선행연구 [NEGATIVE-GUIDED SUBJECT FIDELITY OPTIMIZATION FOR ZERO-SHOT SUBJECT-DRIVEN GENERATION](https://arxiv.org/pdf/2506.03621).  
DPO 라는걸 임커밋 오픈채팅방에서 누가 쏼라쏼라하던거 말로만 듣고 먼지도 모르는 상태에서 실험 결과를보고 안써볼 수 가 없었다. 어차피 학습 파이프라인 0-100 혼자 만들어둔 상태였기에 DPO 학습을 위한 dataloader, loss func 부분 추가하는건 그렇게 어려워 보이지 않아 바로 테스트해본 결과가 위의 이미지다(본인 와꾸임).  






학습 데이터 수집을 위해 외주 맡긴 업체 ~~씨발년들이~~ 핸드폰으로 스크린샷 찍어서 위아래양옆 패딩 되어있고, 작은 이미지 사이즈 upscale해서 화질 개박살난 이미지, group 개판으로 지좆대로 섞어두고 이런 개쓰레기 데이터셋이 거의 반넘게 포진되어있던 터라 필터링하고 분류하는데만 거의 3주 넘게 썼다. 그럼에도 데이터셋이 워낙 개쓰레기라 학습 결과물이 처참했다 (데이터셋 생각하면 지금도 쳐죽이고싶음 ㄹㅇ).  
* 강제 업스케일 된 이미지 영향을 받아 간혹 뿌옇게 된 이미지가 생성됨.
* foundation 모델의 한계로 한국 고유 domain 생성 능력 떨어짐 (한국 모델, 닭발, 뼈해장국, 냉면 등등).
* 상품설명, 조그만 제품이름 같은 작은 한글 텍스트 깨짐 (큰건 그나마 ㄱㅊ)

그런데 이게 왠걸, 먼지도 모르는 DPO 썼더니 한글텍스트 부분 빼고 거의 완치되었음. 이건 도의적으로 읽어줄 수 밖에 없는 논문이었음.  
게다가 요즘 이미지 생성, 이미지 편집 모델은 대부분 학습 후 강화학습으로 한번 bake 해주는게 정배인 터라 꼭 이론적으로 알고있어야하는 연구라고 생각됨.  
근데 논문이 진짜 appendix에 친절하게 하나부터 열까지 차근차근 설명을 잘해줘서 이론 부분은 그렇게 어렵지 않았는데, 실험부분에 세팅을 개같이 꼬아놔서 너무 이해하기 힘들었음. 
* 실험 이해 어렵고 많은거 이건 자연어 논문 종특이라 진짜 짜증나는데 진짜 열심히 읽고 이해했다.

그리고 DPO의 아들인 GRPO도 살짝 맛만 보겠다.

---

그래서 이 포스트는 아래의 4가지에 대한 해답을 얻기 위함임.
* DPO가 그래서 원리가 뭔교?
* 어떻게 해야 극대화 할 수 있는교?
* 이미 DPO, RLHF로 학습 된 모델에 fine-tuning 같은거 해도됨?
* GRPO 얘는 머임

# DPO가 뭐임?
내가 이 논문 읽고 DPO를 한줄로 요약하면 아래와 같다.  

> 모델의 고점은 유지하고 저점을 높여라

RLHF에서 태어난게 DPO이고, RLHF 자체가 "아 생성모델 임마 이거 진짜 빨딱선날은 좋은데 간혹 개 ㅂㅅ이 되네. 이거 ㅂㅅ짓좀 안하게 할수 없노?"를 겨냥하고 연구한걸로 이해했고 DPO도 수식을 보면 ㅂㅅ짓 못하게끔 수식이 구성되어있다.  

![casting.png](/images/DPO/rlhf.png)


# 그래서 DPO 어떻게 쓰라는거임
# DPO 된 foundation 모델에다가 학습 해도됨?
# GRPO
