---
layout: post
title:  "Make-A-Video"
categories: 논문리뷰
date: 2023-06-23 11:40:18 +0900
tags: Diffusion 생성모델 Meta Video
mathjax: True
author: Haribo
---
* content
{:toc}
**Full Citation**: "Singer, Uriel, et al. "Make-a-video: Text-to-video generation without text-video data." arXiv preprint arXiv:2209.14792 (2022)."\
**Link to Paper**: [https://arxiv.org/abs/2209.14792](https://arxiv.org/abs/2209.14792) \
**Conference Details**: ICLR 2023 \
**Project Page**: [Link](https://makeavideo.studio/)

---

>* 선행 Text-to-Video 연구들은 다수의 video-text pair 데이터셋이 필요했으나, 사전학습 된 Diffusion 모델의 능력을 활용해 video-text 없이 video 데이터만을 활용해 고퀄리티 text-to-video 생성모델 학습 방식을 선보임. 
>* 4D 입력인 video 처리를 위해 Spatial/Temporal Convolution + Attention 연산을 활용.



<div style="text-align: center;">   
  <figure>     
    <img src="https://makeavideo.studio/assets/overview.webp">     
  </figure> 
  <figcaption>A dog wearing a Superhero outfit with red cape flying through the sky</figcaption>   
</div>




# 1. Introduction





































